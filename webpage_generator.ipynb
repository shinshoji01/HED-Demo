{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# base_dir = \"./audio/\"\n",
    "base_dir = \"./audiosamples/\"\n",
    "other_dir = \"../seq2seq-vc/datasetgeneration/LLM_responses/08-Others_multi-lingual_text/\"\n",
    "speakers = [str(i).zfill(4) for i in range(11, 21)]\n",
    "\n",
    "def read_file(path):\n",
    "    encodings = ['utf-8', 'utf-16', 'iso-8859-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(path, 'r', encoding=encoding) as file:\n",
    "                return file.readlines(), encoding\n",
    "        except (UnicodeError, UnicodeDecodeError) as e:\n",
    "            continue\n",
    "    raise Exception(\"Failed to decode file with any of the specified encodings.\")\n",
    "    \n",
    "transcription_dir = \"../Dataset/ESD/\"\n",
    "transcriptions = {}\n",
    "emotions = {}\n",
    "for spk in speakers:\n",
    "    path = transcription_dir+spk+f\"/{spk}.txt\"\n",
    "    try:\n",
    "        text, used_encoding = read_file(path)\n",
    "        print(f\"File read successfully with encoding: {used_encoding}\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    text, used_encoding = read_file(path)\n",
    "                \n",
    "    for txt in text:\n",
    "        if len(txt.split())<2:\n",
    "            continue\n",
    "        try:\n",
    "            fn, tcp, emotion = txt[:-1].split(\"\\t\")\n",
    "        except ValueError:\n",
    "            fn = txt.split(\" \")[0]\n",
    "            txt = \" \".join(txt.split(\" \")[1:])\n",
    "            tcp, emotion = txt[:-1].split(\"\\t\")\n",
    "        \n",
    "        transcriptions[fn] = tcp\n",
    "        emotions[fn] = emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "webtitle = \"Hierarchical ED Demo Page\"\n",
    "title = \"Hierarchical Control of Emotion Rendering in Speech Synthesis\"\n",
    "abstract = \"Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional speech from input text. However, quantitatively controlling multi-level emotion rendering remains challenging. In this paper, we propose a diffusion-based emotional TTS framework with a novel approach for emotion intensity modeling to facilitate fine-grained control over emotion rendering at the phoneme, word, and utterance levels. We introduce a hierarchical emotion distribution (ED) extractor that captures a quantifiable ED embedding across different speech segment levels. Additionally, we explore various acoustic features and assess their impact on emotion intensity modeling. During TTS training, the hierarchical ED embedding effectively captures the variance in emotion intensity from the reference audio and correlates it with linguistic and speaker information. During inference, the TTS model not only generates emotional speech but also quantitatively controls the emotion rendering over the speech constituents. Both objective and subjective evaluations demonstrate the effectiveness of our framework in terms of speech quality, emotional expressiveness, and hierarchical emotion control.\"\n",
    "github_url = \"https://github.com/shinshoji01/HED-project-page\"\n",
    "# base_repo_dir = \"/\"\n",
    "base_repo_dir = \"/HED-Demo/\"\n",
    "style_dir = base_repo_dir + \"statics/\"\n",
    "fig_path = base_repo_dir + \"images/emotion_intensity.png\"\n",
    "\n",
    "initial = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n",
    "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n",
    "    <title>{webtitle}</title>\n",
    "    <link href=\"{style_dir}bootstrap-5.2.3-dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    <link href=\"{style_dir}my.css\" rel=\"stylesheet\">\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"container\">\n",
    "      <div class=\"row\">\n",
    "        <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "          <div class=\"text-center\">\n",
    "            <h2>{title}</h2>\n",
    "            <br>\n",
    "            <h5 class=\"mb-4\">Sho Inoue<sup>1,2</sup>, Kun Zhou<sup>3</sup>, Shuai Wang<sup>2†</sup>, Haizhou Li<sup>1,2</sup></h5>\n",
    "              <p>\n",
    "                <sup>1</sup>School of Data Science, <sup>2</sup>Shenzhen Research Institute of Big Data<br>\n",
    "                The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China<br>\n",
    "                <sup>3</sup>Alibaba Group, Singapore\n",
    "              </p>\n",
    "          </div>\n",
    "          <br>\n",
    "          <figure class=\"text-center\">\n",
    "            <img src=\"{fig_path}\" alt=\"overall diagram of the pipeline\" class=\"img-fluid\" style=\"width: 900px; height: auto;\">\n",
    "          </figure>\n",
    "          <br>\n",
    "          <h3>Abstract</h3>\n",
    "          <p class=\"lead\">\n",
    "          {abstract}\n",
    "          </p>\n",
    "          <p class=\"lead\">You can visit the project page of this paper: <a href=\"{github_url}\">Github Repository</a>.\n",
    "          </p>\n",
    "        </div>\n",
    "\"\"\"[1:]\n",
    "\n",
    "closure = f\"\"\"\n",
    "      </div>\n",
    "    </div>\n",
    "    <script src=\"{style_dir}jquery/jquery-3.7.1.slim.min.js\"></script>\n",
    "    <script src=\"{style_dir}bootstrap-5.2.3-dist/bootstrap.min.js\"></script>\n",
    "\"\"\"[1:]\n",
    "closure += \"\"\"\n",
    "  </body>\n",
    "  <script>\n",
    "    $(function(){\n",
    "        $(\"audio\").on(\"play\", function() {\n",
    "            $(\"audio\").not(this).each(function(index, audio) {\n",
    "                audio.pause();\n",
    "                audio.currentTime = 0;\n",
    "            });\n",
    "        });\n",
    "    });\n",
    "    </script>\n",
    "</html>\n",
    "\"\"\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"expressiveness\": {\n",
    "        \"ground_truth\": \"Reference Speech\",\n",
    "        \"expressiveness/msemotts_OpenSMILE_esd\": \"MsEmoTTS (Baseline)\",\n",
    "        \"expressiveness/relative-attributes_OpenSMILE_esd\": \"SVM-based HED (Baseline)\",\n",
    "        \"expressiveness/OpenSMILE-OpenSMILE-WavLM_SER-globaloutput-GRL_esd_fcn\": \"Proposed w/ SER\",\n",
    "        \"expressiveness/OpenSMILE-OpenSMILE-WavLM_EPR-globaloutput-GRL_esd_fcn\": \"Proposed w/ EPR\",\n",
    "    },\n",
    "    \"utterance\": {\n",
    "        \"ground_truth\": \"Ground Truth\",\n",
    "        \"utterance_remain/relative-attributes_OpenSMILE_esd\": \"SVM-based HED (Baseline)\",\n",
    "        \"utterance_remain/OpenSMILE-OpenSMILE-WavLM_SER-globaloutput-GRL_esd_fcn\": \"Proposed w/ SER\",\n",
    "        \"utterance_remain/OpenSMILE-OpenSMILE-WavLM_EPR-globaloutput-GRL_esd_fcn\": \"Proposed w/ EPR\",\n",
    "    },\n",
    "    \"word\": {\n",
    "        \"ground_truth\": \"Ground Truth\",\n",
    "        \"word-phoneme/msemotts_OpenSMILE_esd\": \"MsEmoTTS (Baseline)\",\n",
    "        \"word-phoneme/msemotts_OpenSMILE_esd---cut\": \"___OnlyModified___\",\n",
    "        \"word-phoneme/relative-attributes_OpenSMILE_esd\": \"SVM-based HED (Baseline)\",\n",
    "        \"word-phoneme/relative-attributes_OpenSMILE_esd---cut\": \"___OnlyModified___\",\n",
    "        \"word-phoneme/OpenSMILE-OpenSMILE-WavLM_SER-globaloutput-GRL_esd_fcn\": \"Proposed w/ SER\",\n",
    "        \"word-phoneme/OpenSMILE-OpenSMILE-WavLM_SER-globaloutput-GRL_esd_fcn---cut\": \"___OnlyModified___\",\n",
    "        \"word-phoneme/OpenSMILE-OpenSMILE-WavLM_EPR-globaloutput-GRL_esd_fcn\": \"Proposed w/ EPR\",\n",
    "        \"word-phoneme/OpenSMILE-OpenSMILE-WavLM_EPR-globaloutput-GRL_esd_fcn---cut\": \"___OnlyModified___\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavfiletext(wavfile):\n",
    "    a = f\"\"\"\n",
    "                      <td>\n",
    "    \"\"\"[1:]\n",
    "    a += f\"\"\"\n",
    "                        <audio controls=\"\" preload=\"none\" style=\"width: 240px\">\n",
    "                          <source src=\"{wavfile}\" type=\"audio/wav\">\n",
    "                        </audio>\n",
    "    \"\"\"[1:]\n",
    "    a += f\"\"\"\n",
    "                      </td>\n",
    "    \"\"\"[1:]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.basename(a).split(\"-\")[0] for a in glob.glob(\"./audio/\" + \"expressiveness/relative-attributes_OpenSMILE_esd/00*\")]\n",
    "filenames = list(set(filenames))\n",
    "filenames.sort()\n",
    "np.random.seed(0)\n",
    "filenames = list(np.random.choice(filenames, 20, replace=False))\n",
    "# filenames.sort()\n",
    "controlfns = [\"0014_000732\", \"0015_000726\", \"0013_000731\", \"0018_000037\"]\n",
    "\n",
    "emos = [\"Angry\", \"Happy\", \"Sad\", \"Surprise\"]\n",
    "intensities = [0.0, 0.4, 0.6, 1.0]\n",
    "emotioncolors = {\n",
    "    \"Angry\": \"red\",\n",
    "    \"Happy\": \"orange\",\n",
    "    \"Sad\": \"blue\",\n",
    "    \"Surprise\": \"green\",\n",
    "}\n",
    "modifiedparts = {\n",
    "\"Who is been repeating all that hard stuff to you?\": \"Who is been <u>repeating</u> all that <u>hard</u> <u>stuff</u> to you?\", \n",
    "\"Let's make the noise a snake.\": \"<u>Let's</u> make the <u>noise</u> a <u>snake</u>.\", \n",
    "\"All smile were real and the happier，the more sincere .\": \"All <u>smile</u> were real and the <u>happier</u>，the more <u>sincere</u> .\", \n",
    "\"I think it'll encourage me.\": \"I <u>think</u> <u>it'll</u> <u>encourage</u> me.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\n",
    "    \"expressiveness\": \"Section 1: Emotion Expressiveness: Reproducibility via Ground-Truth Emotion Information (Emotion Transfer)\",\n",
    "    \"utterance\": \"Section 2: Utterance-level Emotion Intensity Control\",\n",
    "    \"word\": \"Section 3: Word-level Emotion Intensity Control\",\n",
    "}\n",
    "explanations = {\n",
    "    \"expressiveness\": \"\\\n",
    "The first section presents a demo to evaluate our model's reproducibility. The Hierarchical ED of our proposed models and the emotion intensities of MsEmoTTS are obtained by the reference audio. We compared the following five samples: \\\n",
    "<ul> \\\n",
    "  <li><b>Reference Speech</b>: The reference audio<br></li> \\\n",
    "  <li><b>MsEmoTTS (Baseline)</b><br></li> \\\n",
    "  <li><b>SVM-based HED (Baseline)</b>: Hierarchical ED is obtained by SVM-based relative functions<br></li> \\\n",
    "  <li><b>Proposed w/ SER</b>: Hierarchical ED is obtained by the proposed SER-based relative functions<br></li> \\\n",
    "  <li><b>Proposed w/ EPR</b>: Hierarchical ED is obtained by the proposed EPR-based relative functions<br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"utterance\": \"\\\n",
    "In this section, we demonstrate the control of utterance-level emotion intensities. We set the emotion intensities of the emotion to the values in the first row while maintaining the other emotion intensities constant. We don't present MsEmoTTS since it does not cover this control. \\\n",
    "<ul> \\\n",
    "  <li><b>Ground Truth</b>: The ground-truth audio<br></li> \\\n",
    "  <li><b>SVM-based HED (Baseline)</b>: Hierarchical ED is obtained by SVM-based relative functions<br></li> \\\n",
    "  <li><b>Proposed w/ SER</b>: Hierarchical ED is obtained by the proposed SER-based relative functions<br></li> \\\n",
    "  <li><b>Proposed w/ EPR</b>: Hierarchical ED is obtained by the proposed EPR-based relative functions<br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"word\": \"\\\n",
    "In this section, we demonstrate the control of emotion intensities in a specific segment of speech, primarily focusing on <u>three words with underscores</u>. We set the word-level emotion intensities of these words, and the corresponding phoneme-level intensities, to the values in the first row, while maintaining the other emotion intensities constant. This showcases our model's ability to adjust emotion granularly within specified speech segments. We also present the speech samples focusing on the modified parts ('___OnlyModified___').<br>\\\n",
    "The MsEmoTTS baseline is only able to change the intensity of the ground-truth emotion so only one emotion control is available. \\\n",
    "<ul> \\\n",
    "  <li><b>Ground Truth</b>: The ground-truth audio<br></li> \\\n",
    "  <li><b>MsEmoTTS (Baseline)</b>: Only one emotion is available.<br></li> \\\n",
    "  <li><b>SVM-based HED (Baseline)</b>: Hierarchical ED is obtained by SVM-based relative functions<br></li> \\\n",
    "  <li><b>Proposed w/ SER</b>: Hierarchical ED is obtained by the proposed SER-based relative functions<br></li> \\\n",
    "  <li><b>Proposed w/ EPR</b>: Hierarchical ED is obtained by the proposed EPR-based relative functions<br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "}\n",
    "filenames_dir = {\n",
    "    \"expressiveness\": filenames,\n",
    "    \"utterance\": controlfns,\n",
    "    \"word\": controlfns,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfiles = []\n",
    "extexts = {}\n",
    "for exid in experiments:\n",
    "# for exid in list(experiments)[:1]:\n",
    "# for exid in list(experiments)[1:]:\n",
    "    text = f\"\"\"\n",
    "            <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "              <h2>{titles[exid]}</h2>\n",
    "              <p class=\"lead\">\n",
    "                    {explanations[exid]}\n",
    "                    </p>\n",
    "    \"\"\"[1:]\n",
    "    emotion_list = emos if exid in [\"utterance\", \"word\"] else [\"noemotion\"]\n",
    "    for emotion in emotion_list:\n",
    "        if exid in [\"utterance\", \"word\"]:\n",
    "            text += f\"\"\"\n",
    "                  <br><hr><br>\n",
    "                  <h4>{exid[0].upper()}{exid[1:]}-level Control: Emotion: <span style='color:{emotioncolors[emotion]};'>{emotion}</span></h4>\n",
    "                  <br>\n",
    "        \"\"\"[1:]\n",
    "        text += f\"\"\"\n",
    "                  <div class=\"table-responsive\" style=\"overflow-x: scroll\">\n",
    "                    <table class=\"table table-sm\">\n",
    "        \"\"\"[1:]\n",
    "        text += f\"\"\"\n",
    "                      <thead>\n",
    "                        <tr>\n",
    "                          <th scope=\"col\">ID</th>\n",
    "        \"\"\"[1:]\n",
    "        for fn in filenames_dir[exid]:\n",
    "            if exid==\"expressiveness\":\n",
    "                repeat = 1\n",
    "            elif exid==\"utterance\":\n",
    "                repeat = len(intensities)\n",
    "            for _ in range(repeat):\n",
    "                if _==0:\n",
    "                    labeltext = fn\n",
    "                else:\n",
    "                    labeltext = \"\"\n",
    "                text += f\"\"\"\n",
    "                          <th scope=\"col\">{labeltext}</th>\n",
    "        \"\"\"[1:]\n",
    "        text += \"\"\"\n",
    "                        </tr>\n",
    "                      </thead>\n",
    "        \"\"\"[1:]\n",
    "\n",
    "        text += \"\"\"\n",
    "                      <tbody>\n",
    "                        <tr>\n",
    "                          <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">Text</th>\n",
    "        \"\"\"[1:]\n",
    "        for fn in filenames_dir[exid]:\n",
    "            if exid==\"expressiveness\":\n",
    "                labeltext = f\"<b>Text</b>: {transcriptions[fn]}<br><b>Emotion</b>: {emotions[fn]}\"\n",
    "                text += f\"\"\"\n",
    "                          <td>\n",
    "                            <p>{labeltext}</p>\n",
    "                          </td>\n",
    "        \"\"\"[1:]\n",
    "            else: \n",
    "                for intensity in intensities:\n",
    "                    if exid==\"utterance\":\n",
    "                        labeltext = f\"<b>Text</b>: {transcriptions[fn]}<br><span style='color:{emotioncolors[emotion]};'><b>Intensity: {intensity}</b></span>\"\n",
    "                    elif exid==\"word\":\n",
    "                        labeltext = f\"<b>Text</b>: {modifiedparts[transcriptions[fn]]}<br><span style='color:{emotioncolors[emotion]};'><b>Intensity: {intensity}</b></span>\"\n",
    "                    text += f\"\"\"\n",
    "                          <td>\n",
    "                            <p>{labeltext}</p>\n",
    "                          </td>\n",
    "        \"\"\"[1:]\n",
    "        text += \"\"\"\n",
    "                        </tr>\n",
    "        \"\"\"[1:]\n",
    "        \n",
    "        for key in experiments[exid]:\n",
    "            text += f\"\"\"\n",
    "                        <tr>\n",
    "                          <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">{experiments[exid][key]}</th>\n",
    "        \"\"\"[1:]\n",
    "            for fn in filenames_dir[exid]:\n",
    "                wavbasefile = base_dir + key.split(\"---\")[0] + f\"/{fn}.wav\"\n",
    "                if exid==\"expressiveness\":\n",
    "                    if os.path.exists(wavbasefile):\n",
    "                        wavfile = wavbasefile\n",
    "                    else:\n",
    "                        wavfile = wavbasefile[:-4] + \"-0.wav\"\n",
    "                    wavfiles += [wavfile]\n",
    "                    text = text + wavfiletext(wavfile)\n",
    "                else:\n",
    "                    for intensity in intensities:\n",
    "                        if os.path.exists(wavbasefile):\n",
    "                            wavfile = wavbasefile\n",
    "                        else:\n",
    "                            if \"---cut\" in key:\n",
    "                                wavfile = wavbasefile[:-4] + f\"-{emotion}-{intensity}-0-cut.wav\"\n",
    "                            else:\n",
    "                                wavfile = wavbasefile[:-4] + f\"-{emotion}-{intensity}-0.wav\"\n",
    "                        if os.path.exists(wavfile):\n",
    "                            wavfiles += [wavfile]\n",
    "                            text = text + wavfiletext(wavfile)\n",
    "                        else:\n",
    "                            text += \"\"\"\n",
    "                          <td>\n",
    "                              <p></p>\n",
    "                          </td>\n",
    "        \"\"\"[1:]\n",
    "            text += \"\"\"\n",
    "                        </tr>\n",
    "        \"\"\"[1:]\n",
    "            \n",
    "        text += \"\"\"\n",
    "                      </tbody>\n",
    "                    </table>\n",
    "                  </div>\n",
    "                  <p class=\"lead\">* please scroll horizontally to explore additional columns in the table.</p>\n",
    "        \"\"\"[1:]\n",
    "    text += \"\"\"\n",
    "                </div>\n",
    "    \"\"\"[1:]\n",
    "    # extexts[exid] = headerfn + headertext + body + tableclosure\n",
    "    # extexts[exid] = headerfn + headertext + text\n",
    "    extexts[exid] = text\n",
    "    extexts[exid] = \"\\n\".join([a[4:] for a in extexts[exid].split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholetext = \"\"\n",
    "wholetext += initial\n",
    "for exid in experiments:\n",
    "# for exid in list(experiments)[:1]:\n",
    "# for exid in list(experiments)[1:]:\n",
    "    wholetext += extexts[exid]\n",
    "wholetext += closure\n",
    "f = open(\"index.html\", \"w\")\n",
    "f.write(wholetext)\n",
    "f.close()\n",
    "print(wholetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "wavfiles = list(set(wavfiles))\n",
    "wavfiles.sort()\n",
    "\n",
    "copy = False\n",
    "tgtdir = \"./audiosamples/\"\n",
    "for path in wavfiles:\n",
    "    src = path.replace(\"audiosamples\", \"audio\")\n",
    "    if copy and os.path.exists(src) and not(os.path.exists(path)):\n",
    "        # print(path)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        shutil.copy(src, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
